{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53d0cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from datasets) (1.21.5)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 KB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 KB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (21.2.0)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: xxhash, tqdm, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, charset_normalizer, async-timeout, aiohappyeyeballs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 charset_normalizer-3.4.3 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 hf-xet-1.1.9 huggingface-hub-0.34.4 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 requests-2.32.5 tqdm-4.67.1 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/lib/python3/dist-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /usr/lib/python3/dist-packages (0.22.0)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2025.8.29-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.9/789.9 KB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 KB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (888.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.0/888.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.27.3\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 KB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch) (2.4)\n",
      "Collecting triton==3.4.0\n",
      "  Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.8.93\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.8.93\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /usr/lib/python3/dist-packages (from torch) (4.10.0)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch) (59.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, torch, torchaudio\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 regex-2025.8.29 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.22.0 torch-2.8.0 torchaudio-2.8.0 transformers-4.56.0 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install transformers torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c59d78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ebea446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME: Not set\n",
      "PATH: /bin:/home/ubuntu/.vscode-server/cli/servers/Stable-c306e94f98122556ca081f527b466015e1bc37b0/server/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/.local/bin:/usr/mpi/gcc/openmpi-4.1.7rc1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/ubuntu/.vscode-server/cli/servers/Stable-c306e94f98122556ca081f527b466015e1bc37b0/server/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/.local/bin:/usr/mpi/gcc/openmpi-4.1.7rc1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LD_LIBRARY_PATH: /usr/mpi/gcc/openmpi-4.1.7rc1/lib:/usr/mpi/gcc/openmpi-4.1.7rc1/lib64\n",
      "PyTorch version: 2.8.0+cu128\n",
      "PyTorch CUDA version: 12.8\n",
      "Is CUDA build: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CUDA_HOME:\", os.environ.get('CUDA_HOME', 'Not set'))\n",
    "print(\"PATH:\", os.environ.get('PATH'))\n",
    "print(\"LD_LIBRARY_PATH:\", os.environ.get('LD_LIBRARY_PATH', 'Not set'))\n",
    "\n",
    "# Check PyTorch installation\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Is CUDA build: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbfdc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EMBED_DIM = 256\n",
    "NUM_ITERS = 4\n",
    "ALPHA = 0.5\n",
    "LR = 5e-5\n",
    "EPOCHS = 10\n",
    "MAX_LENGTH = 4096 # Maximum token length for padding/truncation\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_TO_MULTIPLE_OF=8\n",
    "GRADIENT_CLIPPING = 1.0\n",
    "\n",
    "# Test if CUDA is available\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d22c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News dataset\n",
    "dataset = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161a80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                          padding=\"max_length\",\n",
    "                                          truncation=True,\n",
    "                                          max_length=MAX_LENGTH,\n",
    "                                          pad_to_multiple_of=PAD_TO_MULTIPLE_OF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e2090b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7574e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b225ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = label_encoder.transform(dataset['train']['label'])\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = label_encoder.transform(dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab969d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AGNewsDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=12, pin_memory=True,\n",
    "                          prefetch_factor=4,\n",
    "                          persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edcb3c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a validation set before training start\n",
    "import random\n",
    "\n",
    "# Select a small random subset from our test dataset\n",
    "subset_size = 10  # Adjust as needed\n",
    "subset_indices = random.sample(range(len(test_loader.dataset)), subset_size)\n",
    "\n",
    "# Create a new DataLoader for this subset\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "test_subset = Subset(test_loader.dataset, subset_indices)\n",
    "test_subset_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665a9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 2: Define the Model ==========\n",
    "class DiffusionAttentionFreeModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_iters=NUM_ITERS, alpha=ALPHA, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.noise_std = 0.1  # Initial noise\n",
    "        self.alpha = alpha  # Decay factor\n",
    "        self.num_iters = num_iters  # Iterative updates\n",
    "        self.update_mlp = nn.Linear(embed_dim, embed_dim)  # Local transformation\n",
    "        self.output_mlp = nn.Linear(embed_dim, num_classes)  # Classifier\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Step 1: Embed + Add Noise\n",
    "        h = self.embedding(input_ids) + self.noise_std * torch.randn_like(self.embedding(input_ids))\n",
    "\n",
    "        # Step 2: Iterative Refinement (Diffusion Process)\n",
    "        for _ in range(self.num_iters):\n",
    "            # Multi-Neighbor Updates\n",
    "            h_left = torch.roll(h, shifts=1, dims=1)\n",
    "            h_right = torch.roll(h, shifts=-1, dims=1)\n",
    "            h_update = self.update_mlp(h_left) + self.update_mlp(h_right)\n",
    "\n",
    "            # Weighted update rule (diffusion-like)\n",
    "            h = self.alpha * h + (1 - self.alpha) * h_update\n",
    "\n",
    "        # Step 3: Pooling + Classification\n",
    "        h = (h * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)  # Masked mean pooling\n",
    "        logits = self.output_mlp(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab7566b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(test_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa18750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate 5e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning rate\", LR)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "diff_model = DiffusionAttentionFreeModel(vocab_size, EMBED_DIM).to(DEVICE)\n",
    "optimizer = optim.AdamW(diff_model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "443d1e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Load Time = 0.0002 sec\n",
      "Batch 2: Load Time = 0.0001 sec\n",
      "Batch 3: Load Time = 0.0001 sec\n",
      "Batch 4: Load Time = 0.0001 sec\n",
      "Batch 5: Load Time = 0.0002 sec\n",
      "Batch 6: Load Time = 0.0002 sec\n",
      "Batch 7: Load Time = 0.0002 sec\n",
      "Batch 8: Load Time = 0.0001 sec\n",
      "Batch 9: Load Time = 0.0002 sec\n",
      "Batch 10: Load Time = 0.0003 sec\n",
      "Batch 11: Load Time = 0.0001 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i, batch in enumerate(train_loader):\n",
    "    start_time = time.time()\n",
    "    batch_data = batch[\"input_ids\"].to(DEVICE)  # Load batch to GPU\n",
    "    print(f\"Batch {i+1}: Load Time = {time.time() - start_time:.4f} sec\")\n",
    "\n",
    "    if i == 10:  # Stop after 10 batches\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19be5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDiffusionAttentionFreeModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_iters=4, alpha=0.7, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_iters = num_iters\n",
    "        self.alpha = alpha\n",
    "        self.noise_std = 0.05  # Reduced for FP16 stability\n",
    "        \n",
    "        # Token embedding with proper initialization\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Multi-head neighbor interaction (more sophisticated than single MLP)\n",
    "        self.neighbor_proj = nn.ModuleList([\n",
    "            nn.Linear(embed_dim, embed_dim, bias=False) for _ in range(3)\n",
    "        ])  # left, right, self projections\n",
    "        \n",
    "        # Layer normalization for stability\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Nonlinear transformation with residual connection\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Classification head with dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for FP16 stability\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Proper weight initialization for FP16 training\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Xavier initialization scaled for FP16\n",
    "                nn.init.xavier_normal_(module.weight, gain=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                nn.init.constant_(module.weight, 1.0)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Step 1: Embed + Add controlled noise\n",
    "        h = self.embedding(input_ids)\n",
    "        \n",
    "        if self.training:\n",
    "            # Add noise only during training, with proper scaling\n",
    "            noise = torch.randn_like(h, dtype=h.dtype, device=h.device) * self.noise_std\n",
    "            h = h + noise\n",
    "        \n",
    "        # Step 2: Iterative refinement with proper neighbor handling\n",
    "        for iteration in range(self.num_iters):\n",
    "            # Get neighbor representations\n",
    "            h_left = torch.cat([h[:, -1:, :], h[:, :-1, :]], dim=1)  # Proper circular shift\n",
    "            h_right = torch.cat([h[:, 1:, :], h[:, :1, :]], dim=1)   # Proper circular shift\n",
    "            \n",
    "            # Apply different projections to each neighbor type\n",
    "            h_left_proj = self.neighbor_proj[0](h_left)\n",
    "            h_right_proj = self.neighbor_proj[1](h_right)\n",
    "            h_self_proj = self.neighbor_proj[2](h)\n",
    "            \n",
    "            # Combine neighbor information\n",
    "            neighbor_sum = h_left_proj + h_right_proj + h_self_proj\n",
    "            \n",
    "            # Apply nonlinear transformation\n",
    "            h_update = self.update_mlp(neighbor_sum)\n",
    "            \n",
    "            # Residual connection + weighted update\n",
    "            h_new = self.alpha * h + (1 - self.alpha) * h_update\n",
    "            \n",
    "            # Apply layer normalization for stability\n",
    "            h = self.layer_norm(h_new)\n",
    "        \n",
    "        # Step 3: Masked pooling (handle padding properly)\n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask for broadcasting\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "            h_masked = h * mask_expanded\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            mask_sum = mask_expanded.sum(dim=1).clamp(min=1e-8)\n",
    "            pooled = h_masked.sum(dim=1) / mask_sum\n",
    "        else:\n",
    "            pooled = h.mean(dim=1)\n",
    "        \n",
    "        # Step 4: Classification\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f821521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_model(model, train_loader, test_loader, device, \n",
    "                        epochs=50, lr=5e-5, checkpoint_path=\"./checkpoints\"):\n",
    "    \"\"\"\n",
    "    Training function with proper FP16 support and gradient scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize optimizer with FP16-friendly settings\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr,\n",
    "        weight_decay=0.01,\n",
    "        eps=1e-8  # Increased epsilon for FP16 numerical stability\n",
    "    )\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision\n",
    "    scaler = GradScaler('cuda')\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=5 \n",
    "    )\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    latest_checkpoint = os.path.join(checkpoint_path, \"latest_model.pth\")\n",
    "    initial_epoch = 1\n",
    "    \n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(latest_checkpoint, weights_only=False)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "        initial_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Resuming training from epoch {initial_epoch}\")\n",
    "    \n",
    "    # Training loop\n",
    "    log_file = os.path.join(checkpoint_path, f\"training_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "    \n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"=== Training Start - {datetime.datetime.now()} ===\\n\")\n",
    "        f.write(f\"Model: ImprovedDiffusionAttentionFreeModel\\n\")\n",
    "        f.write(f\"Learning Rate: {lr}\\n\")\n",
    "        f.write(f\"Epochs: {epochs}\\n\")\n",
    "        f.write(f\"Initial Scaler Scale: {scaler.get_scale()}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        \n",
    "        for epoch in range(initial_epoch, epochs + 1):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass with autocast for mixed precision\n",
    "                with autocast('cuda'):\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Check for loss explosion early\n",
    "                if loss.item() > 100:\n",
    "                    print(f\"WARNING: Loss explosion detected: {loss.item():.2f}\")\n",
    "                    print(f\"Scaler scale: {scaler.get_scale()}\")\n",
    "                    f.write(f\"WARNING: Loss explosion at epoch {epoch}, batch {batch_idx}\\n\")\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping (unscale first)\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Optimizer step\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                # Statistics\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # Log progress every 50 batches\n",
    "                if batch_idx % 50 == 0:\n",
    "                    current_acc = 100. * correct / total_samples\n",
    "                    print(f\"Epoch {epoch}, Batch {batch_idx}: Loss = {loss.item():.4f}, Acc = {current_acc:.2f}%\")\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            train_accuracy = correct / total_samples\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Evaluation phase\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    with autocast('cuda'):\n",
    "                        outputs = model(input_ids, attention_mask)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    test_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    test_total += labels.size(0)\n",
    "                    test_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            test_accuracy = test_correct / test_total\n",
    "            avg_test_loss = test_loss / len(test_loader)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_test_loss)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scaler_state_dict\": scaler.state_dict(),\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"test_loss\": avg_test_loss,\n",
    "                \"train_acc\": train_accuracy,\n",
    "                \"test_acc\": test_accuracy,\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, latest_checkpoint)\n",
    "            if epoch % 5 == 0:  # Save every 5 epochs\n",
    "                torch.save(checkpoint, os.path.join(checkpoint_path, f\"model_epoch_{epoch}.pth\"))\n",
    "            \n",
    "            # Logging\n",
    "            log_msg = f\"Epoch {epoch}/{epochs}:\\n\"\n",
    "            log_msg += f\"  Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}\\n\"\n",
    "            log_msg += f\"  Test  - Loss: {avg_test_loss:.4f}, Acc: {test_accuracy:.4f}\\n\"\n",
    "            log_msg += f\"  Time: {epoch_time:.2f}s, LR: {current_lr:.2e}\\n\"\n",
    "            log_msg += f\"  Scaler Scale: {scaler.get_scale()}\\n\"\n",
    "            log_msg += \"-\" * 50 + \"\\n\"\n",
    "            \n",
    "            print(log_msg)\n",
    "            f.write(log_msg)\n",
    "            f.flush()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "718c65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_improved_training():\n",
    "    \"\"\"\n",
    "    Setup function to replace your current training loop\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your existing hyperparameters\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    EMBED_DIM = 256\n",
    "    NUM_ITERS = 4\n",
    "    ALPHA = 0.7  # Increased from 0.5 for better stability\n",
    "    LR = 2e-5    # Reduced from 5e-5 for FP16 stability\n",
    "    EPOCHS = 5\n",
    "    \n",
    "    # Initialize improved model\n",
    "    vocab_size = 30522  # Your tokenizer vocab size\n",
    "    model = ImprovedDiffusionAttentionFreeModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        num_iters=NUM_ITERS,\n",
    "        alpha=ALPHA,\n",
    "        num_classes=4\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Convert to half precision\n",
    "    model = model.half()\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Model size (MB): {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.2f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8f82caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, tokenizer):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for your current training cell\n",
    "    \"\"\"\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create improved model\n",
    "    model = ImprovedDiffusionAttentionFreeModel(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_dim=256,  # Your EMBED_DIM\n",
    "        num_iters=4,    # Your NUM_ITERS  \n",
    "        alpha=0.7,      # Improved from your 0.5\n",
    "        num_classes=4\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Convert to FP16 - Removing this for the time being\n",
    "    #model = model.half()\n",
    "    \n",
    "    # Start training with proper FP16 support\n",
    "    trained_model = train_improved_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=DEVICE,\n",
    "        epochs=5,\n",
    "        lr=2e-5,  # Reduced for FP16 stability\n",
    "        checkpoint_path=\"./checkpoints_improved\"\n",
    "    )\n",
    "    \n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59247c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0: Loss = 1.3867, Acc = 31.25%\n",
      "Epoch 1, Batch 50: Loss = 1.3865, Acc = 24.02%\n",
      "Epoch 1, Batch 100: Loss = 1.3865, Acc = 25.43%\n",
      "Epoch 1, Batch 150: Loss = 1.3862, Acc = 25.95%\n",
      "Epoch 1, Batch 200: Loss = 1.3860, Acc = 26.21%\n",
      "Epoch 1, Batch 250: Loss = 1.3856, Acc = 28.21%\n",
      "Epoch 1, Batch 300: Loss = 1.3859, Acc = 29.98%\n",
      "Epoch 1, Batch 350: Loss = 1.3843, Acc = 31.53%\n",
      "Epoch 1, Batch 400: Loss = 1.2997, Acc = 33.25%\n",
      "Epoch 1, Batch 450: Loss = 1.2580, Acc = 34.23%\n",
      "Epoch 1, Batch 500: Loss = 1.0266, Acc = 35.00%\n",
      "Epoch 1, Batch 550: Loss = 1.0665, Acc = 35.99%\n",
      "Epoch 1, Batch 600: Loss = 1.0921, Acc = 36.80%\n",
      "Epoch 1, Batch 650: Loss = 0.9820, Acc = 37.46%\n",
      "Epoch 1, Batch 700: Loss = 0.8188, Acc = 38.02%\n",
      "Epoch 1, Batch 750: Loss = 0.7926, Acc = 38.80%\n",
      "Epoch 1, Batch 800: Loss = 0.9577, Acc = 39.39%\n",
      "Epoch 1, Batch 850: Loss = 1.2787, Acc = 39.81%\n",
      "Epoch 1, Batch 900: Loss = 1.1305, Acc = 40.03%\n",
      "Epoch 1, Batch 950: Loss = 1.1370, Acc = 40.35%\n",
      "Epoch 1, Batch 1000: Loss = 1.1495, Acc = 40.73%\n",
      "Epoch 1, Batch 1050: Loss = 1.1254, Acc = 40.96%\n",
      "Epoch 1, Batch 1100: Loss = 1.2167, Acc = 41.28%\n",
      "Epoch 1, Batch 1150: Loss = 0.8080, Acc = 41.46%\n",
      "Epoch 1, Batch 1200: Loss = 0.8364, Acc = 41.64%\n",
      "Epoch 1, Batch 1250: Loss = 0.9829, Acc = 41.76%\n",
      "Epoch 1, Batch 1300: Loss = 0.9446, Acc = 42.06%\n",
      "Epoch 1, Batch 1350: Loss = 0.7462, Acc = 42.45%\n",
      "Epoch 1, Batch 1400: Loss = 1.0123, Acc = 42.69%\n",
      "Epoch 1, Batch 1450: Loss = 0.9944, Acc = 42.82%\n",
      "Epoch 1, Batch 1500: Loss = 0.8443, Acc = 42.96%\n",
      "Epoch 1, Batch 1550: Loss = 0.9736, Acc = 43.08%\n",
      "Epoch 1, Batch 1600: Loss = 0.9588, Acc = 43.20%\n",
      "Epoch 1, Batch 1650: Loss = 1.0405, Acc = 43.41%\n",
      "Epoch 1, Batch 1700: Loss = 0.9616, Acc = 43.51%\n",
      "Epoch 1, Batch 1750: Loss = 0.9985, Acc = 43.64%\n",
      "Epoch 1, Batch 1800: Loss = 1.0715, Acc = 43.78%\n",
      "Epoch 1, Batch 1850: Loss = 1.1066, Acc = 43.94%\n",
      "Epoch 1, Batch 1900: Loss = 0.9448, Acc = 44.09%\n",
      "Epoch 1, Batch 1950: Loss = 0.9558, Acc = 44.22%\n",
      "Epoch 1, Batch 2000: Loss = 0.8261, Acc = 44.41%\n",
      "Epoch 1, Batch 2050: Loss = 1.0197, Acc = 44.44%\n",
      "Epoch 1, Batch 2100: Loss = 0.8904, Acc = 44.54%\n",
      "Epoch 1, Batch 2150: Loss = 0.7155, Acc = 44.71%\n",
      "Epoch 1, Batch 2200: Loss = 1.1624, Acc = 44.79%\n",
      "Epoch 1, Batch 2250: Loss = 0.9693, Acc = 44.98%\n",
      "Epoch 1, Batch 2300: Loss = 0.9742, Acc = 45.13%\n",
      "Epoch 1, Batch 2350: Loss = 0.7305, Acc = 45.26%\n",
      "Epoch 1, Batch 2400: Loss = 0.9614, Acc = 45.38%\n",
      "Epoch 1, Batch 2450: Loss = 1.0789, Acc = 45.55%\n",
      "Epoch 1, Batch 2500: Loss = 0.9181, Acc = 45.76%\n",
      "Epoch 1, Batch 2550: Loss = 0.7840, Acc = 45.92%\n",
      "Epoch 1, Batch 2600: Loss = 0.8494, Acc = 46.04%\n",
      "Epoch 1, Batch 2650: Loss = 0.7559, Acc = 46.20%\n",
      "Epoch 1, Batch 2700: Loss = 0.7758, Acc = 46.37%\n",
      "Epoch 1, Batch 2750: Loss = 0.7503, Acc = 46.51%\n",
      "Epoch 1, Batch 2800: Loss = 0.7699, Acc = 46.60%\n",
      "Epoch 1, Batch 2850: Loss = 0.9904, Acc = 46.70%\n",
      "Epoch 1, Batch 2900: Loss = 0.9176, Acc = 46.82%\n",
      "Epoch 1, Batch 2950: Loss = 1.0705, Acc = 46.95%\n",
      "Epoch 1, Batch 3000: Loss = 0.8412, Acc = 47.06%\n",
      "Epoch 1, Batch 3050: Loss = 0.6979, Acc = 47.20%\n",
      "Epoch 1, Batch 3100: Loss = 0.9123, Acc = 47.31%\n",
      "Epoch 1, Batch 3150: Loss = 0.6765, Acc = 47.46%\n",
      "Epoch 1, Batch 3200: Loss = 0.7370, Acc = 47.58%\n",
      "Epoch 1, Batch 3250: Loss = 0.9942, Acc = 47.69%\n",
      "Epoch 1, Batch 3300: Loss = 0.8129, Acc = 47.85%\n",
      "Epoch 1, Batch 3350: Loss = 0.7181, Acc = 47.96%\n",
      "Epoch 1, Batch 3400: Loss = 0.5577, Acc = 48.08%\n",
      "Epoch 1, Batch 3450: Loss = 1.0223, Acc = 48.16%\n",
      "Epoch 1, Batch 3500: Loss = 0.9014, Acc = 48.30%\n",
      "Epoch 1, Batch 3550: Loss = 0.8940, Acc = 48.41%\n",
      "Epoch 1, Batch 3600: Loss = 0.9802, Acc = 48.54%\n",
      "Epoch 1, Batch 3650: Loss = 0.8498, Acc = 48.63%\n",
      "Epoch 1, Batch 3700: Loss = 0.8473, Acc = 48.73%\n",
      "Epoch 1, Batch 3750: Loss = 0.6605, Acc = 48.81%\n",
      "Epoch 1, Batch 3800: Loss = 0.9019, Acc = 48.90%\n",
      "Epoch 1, Batch 3850: Loss = 0.7945, Acc = 49.02%\n",
      "Epoch 1, Batch 3900: Loss = 0.6872, Acc = 49.14%\n",
      "Epoch 1, Batch 3950: Loss = 0.9609, Acc = 49.22%\n",
      "Epoch 1, Batch 4000: Loss = 0.8514, Acc = 49.33%\n",
      "Epoch 1, Batch 4050: Loss = 0.7762, Acc = 49.43%\n",
      "Epoch 1, Batch 4100: Loss = 1.4833, Acc = 49.53%\n",
      "Epoch 1, Batch 4150: Loss = 0.8535, Acc = 49.63%\n",
      "Epoch 1, Batch 4200: Loss = 0.7841, Acc = 49.75%\n",
      "Epoch 1, Batch 4250: Loss = 0.9818, Acc = 49.85%\n",
      "Epoch 1, Batch 4300: Loss = 0.9131, Acc = 49.93%\n",
      "Epoch 1, Batch 4350: Loss = 0.9562, Acc = 50.00%\n",
      "Epoch 1, Batch 4400: Loss = 0.8068, Acc = 50.08%\n",
      "Epoch 1, Batch 4450: Loss = 0.7563, Acc = 50.14%\n",
      "Epoch 1, Batch 4500: Loss = 0.6619, Acc = 50.23%\n",
      "Epoch 1, Batch 4550: Loss = 0.8647, Acc = 50.28%\n",
      "Epoch 1, Batch 4600: Loss = 0.8334, Acc = 50.37%\n",
      "Epoch 1, Batch 4650: Loss = 0.6907, Acc = 50.44%\n",
      "Epoch 1, Batch 4700: Loss = 1.3251, Acc = 50.51%\n",
      "Epoch 1, Batch 4750: Loss = 0.9407, Acc = 50.60%\n",
      "Epoch 1, Batch 4800: Loss = 0.6540, Acc = 50.71%\n",
      "Epoch 1, Batch 4850: Loss = 1.0166, Acc = 50.79%\n",
      "Epoch 1, Batch 4900: Loss = 0.7024, Acc = 50.88%\n",
      "Epoch 1, Batch 4950: Loss = 0.7765, Acc = 50.97%\n",
      "Epoch 1, Batch 5000: Loss = 0.7163, Acc = 51.09%\n",
      "Epoch 1, Batch 5050: Loss = 0.8597, Acc = 51.22%\n",
      "Epoch 1, Batch 5100: Loss = 0.8905, Acc = 51.27%\n",
      "Epoch 1, Batch 5150: Loss = 0.6787, Acc = 51.37%\n",
      "Epoch 1, Batch 5200: Loss = 0.6352, Acc = 51.43%\n",
      "Epoch 1, Batch 5250: Loss = 0.6300, Acc = 51.52%\n",
      "Epoch 1, Batch 5300: Loss = 0.6674, Acc = 51.62%\n",
      "Epoch 1, Batch 5350: Loss = 1.1609, Acc = 51.68%\n",
      "Epoch 1, Batch 5400: Loss = 0.7905, Acc = 51.77%\n",
      "Epoch 1, Batch 5450: Loss = 0.5433, Acc = 51.87%\n",
      "Epoch 1, Batch 5500: Loss = 0.9070, Acc = 51.93%\n",
      "Epoch 1, Batch 5550: Loss = 0.7911, Acc = 52.01%\n",
      "Epoch 1, Batch 5600: Loss = 1.0425, Acc = 52.10%\n",
      "Epoch 1, Batch 5650: Loss = 0.6260, Acc = 52.17%\n",
      "Epoch 1, Batch 5700: Loss = 0.6718, Acc = 52.25%\n",
      "Epoch 1, Batch 5750: Loss = 0.7967, Acc = 52.29%\n",
      "Epoch 1, Batch 5800: Loss = 0.8015, Acc = 52.35%\n",
      "Epoch 1, Batch 5850: Loss = 0.6170, Acc = 52.40%\n",
      "Epoch 1, Batch 5900: Loss = 0.6006, Acc = 52.46%\n",
      "Epoch 1, Batch 5950: Loss = 0.7427, Acc = 52.53%\n",
      "Epoch 1, Batch 6000: Loss = 0.5711, Acc = 52.61%\n",
      "Epoch 1, Batch 6050: Loss = 0.8174, Acc = 52.67%\n",
      "Epoch 1, Batch 6100: Loss = 1.2755, Acc = 52.75%\n",
      "Epoch 1, Batch 6150: Loss = 1.2818, Acc = 52.79%\n",
      "Epoch 1, Batch 6200: Loss = 0.7054, Acc = 52.87%\n",
      "Epoch 1, Batch 6250: Loss = 0.6594, Acc = 52.96%\n",
      "Epoch 1, Batch 6300: Loss = 0.8388, Acc = 53.02%\n",
      "Epoch 1, Batch 6350: Loss = 0.6458, Acc = 53.09%\n",
      "Epoch 1, Batch 6400: Loss = 0.6040, Acc = 53.17%\n",
      "Epoch 1, Batch 6450: Loss = 1.2961, Acc = 53.22%\n",
      "Epoch 1, Batch 6500: Loss = 0.9524, Acc = 53.27%\n",
      "Epoch 1, Batch 6550: Loss = 0.8266, Acc = 53.34%\n",
      "Epoch 1, Batch 6600: Loss = 0.7507, Acc = 53.40%\n",
      "Epoch 1, Batch 6650: Loss = 0.7272, Acc = 53.47%\n",
      "Epoch 1, Batch 6700: Loss = 0.7458, Acc = 53.53%\n",
      "Epoch 1, Batch 6750: Loss = 1.0049, Acc = 53.59%\n",
      "Epoch 1, Batch 6800: Loss = 0.6171, Acc = 53.67%\n",
      "Epoch 1, Batch 6850: Loss = 0.6255, Acc = 53.72%\n",
      "Epoch 1, Batch 6900: Loss = 0.6767, Acc = 53.80%\n",
      "Epoch 1, Batch 6950: Loss = 0.7747, Acc = 53.86%\n",
      "Epoch 1, Batch 7000: Loss = 0.8476, Acc = 53.90%\n",
      "Epoch 1, Batch 7050: Loss = 0.5568, Acc = 53.96%\n",
      "Epoch 1, Batch 7100: Loss = 0.7066, Acc = 54.03%\n",
      "Epoch 1, Batch 7150: Loss = 0.5841, Acc = 54.10%\n",
      "Epoch 1, Batch 7200: Loss = 0.9147, Acc = 54.16%\n",
      "Epoch 1, Batch 7250: Loss = 0.7713, Acc = 54.22%\n",
      "Epoch 1, Batch 7300: Loss = 0.4917, Acc = 54.28%\n",
      "Epoch 1, Batch 7350: Loss = 0.8679, Acc = 54.34%\n",
      "Epoch 1, Batch 7400: Loss = 0.6185, Acc = 54.40%\n",
      "Epoch 1, Batch 7450: Loss = 0.5163, Acc = 54.46%\n",
      "Epoch 1/5:\n",
      "  Train - Loss: 0.8886, Acc: 0.5452\n",
      "  Test  - Loss: 0.7026, Acc: 0.6529\n",
      "  Time: 218.44s, LR: 2.00e-05\n",
      "  Scaler Scale: 32768.0\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 2, Batch 0: Loss = 0.6669, Acc = 81.25%\n",
      "Epoch 2, Batch 50: Loss = 0.7624, Acc = 64.71%\n",
      "Epoch 2, Batch 100: Loss = 0.5963, Acc = 63.86%\n",
      "Epoch 2, Batch 150: Loss = 0.5902, Acc = 64.69%\n",
      "Epoch 2, Batch 200: Loss = 0.7980, Acc = 64.24%\n",
      "Epoch 2, Batch 250: Loss = 0.5907, Acc = 64.59%\n",
      "Epoch 2, Batch 300: Loss = 0.7350, Acc = 64.60%\n",
      "Epoch 2, Batch 350: Loss = 0.8059, Acc = 64.55%\n",
      "Epoch 2, Batch 400: Loss = 0.7096, Acc = 64.56%\n",
      "Epoch 2, Batch 450: Loss = 0.7785, Acc = 65.16%\n",
      "Epoch 2, Batch 500: Loss = 0.4725, Acc = 65.21%\n",
      "Epoch 2, Batch 550: Loss = 0.4332, Acc = 65.39%\n",
      "Epoch 2, Batch 600: Loss = 0.5375, Acc = 65.22%\n",
      "Epoch 2, Batch 650: Loss = 0.6004, Acc = 65.17%\n",
      "Epoch 2, Batch 700: Loss = 0.6764, Acc = 65.20%\n",
      "Epoch 2, Batch 750: Loss = 0.7184, Acc = 65.15%\n",
      "Epoch 2, Batch 800: Loss = 0.9767, Acc = 65.08%\n",
      "Epoch 2, Batch 850: Loss = 0.9285, Acc = 65.03%\n",
      "Epoch 2, Batch 900: Loss = 0.3104, Acc = 64.93%\n",
      "Epoch 2, Batch 950: Loss = 0.6052, Acc = 64.93%\n",
      "Epoch 2, Batch 1000: Loss = 0.6173, Acc = 65.07%\n",
      "Epoch 2, Batch 1050: Loss = 0.4587, Acc = 65.10%\n",
      "Epoch 2, Batch 1100: Loss = 0.6342, Acc = 65.17%\n",
      "Epoch 2, Batch 1150: Loss = 0.6511, Acc = 65.35%\n",
      "Epoch 2, Batch 1200: Loss = 0.5459, Acc = 65.40%\n",
      "Epoch 2, Batch 1250: Loss = 0.5586, Acc = 65.58%\n",
      "Epoch 2, Batch 1300: Loss = 1.1011, Acc = 65.53%\n",
      "Epoch 2, Batch 1350: Loss = 0.6293, Acc = 65.59%\n",
      "Epoch 2, Batch 1400: Loss = 0.8488, Acc = 65.52%\n",
      "Epoch 2, Batch 1450: Loss = 0.5523, Acc = 65.61%\n",
      "Epoch 2, Batch 1500: Loss = 0.4272, Acc = 65.62%\n",
      "Epoch 2, Batch 1550: Loss = 0.5054, Acc = 65.79%\n",
      "Epoch 2, Batch 1600: Loss = 0.5218, Acc = 65.78%\n",
      "Epoch 2, Batch 1650: Loss = 0.9796, Acc = 65.93%\n",
      "Epoch 2, Batch 1700: Loss = 0.4672, Acc = 65.94%\n",
      "Epoch 2, Batch 1750: Loss = 0.8337, Acc = 65.98%\n",
      "Epoch 2, Batch 1800: Loss = 0.6933, Acc = 66.06%\n",
      "Epoch 2, Batch 1850: Loss = 0.5201, Acc = 66.07%\n",
      "Epoch 2, Batch 1900: Loss = 0.8070, Acc = 66.16%\n",
      "Epoch 2, Batch 1950: Loss = 0.6313, Acc = 66.22%\n",
      "Epoch 2, Batch 2000: Loss = 0.3701, Acc = 66.35%\n",
      "Epoch 2, Batch 2050: Loss = 0.6834, Acc = 66.40%\n",
      "Epoch 2, Batch 2100: Loss = 0.4142, Acc = 66.51%\n",
      "Epoch 2, Batch 2150: Loss = 0.3879, Acc = 66.56%\n",
      "Epoch 2, Batch 2200: Loss = 0.4493, Acc = 66.64%\n",
      "Epoch 2, Batch 2250: Loss = 0.6322, Acc = 66.76%\n",
      "Epoch 2, Batch 2300: Loss = 0.4440, Acc = 66.88%\n",
      "Epoch 2, Batch 2350: Loss = 0.5187, Acc = 66.95%\n",
      "Epoch 2, Batch 2400: Loss = 0.5736, Acc = 67.03%\n",
      "Epoch 2, Batch 2450: Loss = 0.5118, Acc = 67.07%\n",
      "Epoch 2, Batch 2500: Loss = 0.8239, Acc = 67.17%\n",
      "Epoch 2, Batch 2550: Loss = 0.4419, Acc = 67.26%\n",
      "Epoch 2, Batch 2600: Loss = 0.4473, Acc = 67.38%\n",
      "Epoch 2, Batch 2650: Loss = 0.4785, Acc = 67.53%\n",
      "Epoch 2, Batch 2700: Loss = 0.5797, Acc = 67.63%\n",
      "Epoch 2, Batch 2750: Loss = 0.3303, Acc = 67.71%\n",
      "Epoch 2, Batch 2800: Loss = 0.6476, Acc = 67.80%\n",
      "Epoch 2, Batch 2850: Loss = 0.7426, Acc = 67.85%\n",
      "Epoch 2, Batch 2900: Loss = 0.6005, Acc = 67.93%\n",
      "Epoch 2, Batch 2950: Loss = 0.5473, Acc = 67.97%\n",
      "Epoch 2, Batch 3000: Loss = 0.7039, Acc = 68.04%\n",
      "Epoch 2, Batch 3050: Loss = 0.4252, Acc = 68.17%\n",
      "Epoch 2, Batch 3100: Loss = 0.8893, Acc = 68.29%\n",
      "Epoch 2, Batch 3150: Loss = 0.8851, Acc = 68.41%\n",
      "Epoch 2, Batch 3200: Loss = 0.5724, Acc = 68.52%\n",
      "Epoch 2, Batch 3250: Loss = 0.1914, Acc = 68.63%\n",
      "Epoch 2, Batch 3300: Loss = 0.5138, Acc = 68.75%\n",
      "Epoch 2, Batch 3350: Loss = 0.4837, Acc = 68.86%\n",
      "Epoch 2, Batch 3400: Loss = 0.5324, Acc = 68.98%\n",
      "Epoch 2, Batch 3450: Loss = 0.3816, Acc = 69.10%\n",
      "Epoch 2, Batch 3500: Loss = 0.4941, Acc = 69.24%\n",
      "Epoch 2, Batch 3550: Loss = 0.3803, Acc = 69.31%\n",
      "Epoch 2, Batch 3600: Loss = 0.4938, Acc = 69.42%\n",
      "Epoch 2, Batch 3650: Loss = 0.3831, Acc = 69.51%\n",
      "Epoch 2, Batch 3700: Loss = 0.4117, Acc = 69.64%\n",
      "Epoch 2, Batch 3750: Loss = 0.5912, Acc = 69.75%\n",
      "Epoch 2, Batch 3800: Loss = 0.2330, Acc = 69.88%\n",
      "Epoch 2, Batch 3850: Loss = 0.3940, Acc = 70.00%\n",
      "Epoch 2, Batch 3900: Loss = 0.9297, Acc = 70.10%\n",
      "Epoch 2, Batch 3950: Loss = 0.3272, Acc = 70.22%\n",
      "Epoch 2, Batch 4000: Loss = 0.4305, Acc = 70.37%\n",
      "Epoch 2, Batch 4050: Loss = 0.6340, Acc = 70.49%\n",
      "Epoch 2, Batch 4100: Loss = 0.3725, Acc = 70.59%\n",
      "Epoch 2, Batch 4150: Loss = 0.4862, Acc = 70.70%\n",
      "Epoch 2, Batch 4200: Loss = 0.5926, Acc = 70.80%\n",
      "Epoch 2, Batch 4250: Loss = 0.4931, Acc = 70.92%\n",
      "Epoch 2, Batch 4300: Loss = 0.2482, Acc = 71.03%\n",
      "Epoch 2, Batch 4350: Loss = 0.6742, Acc = 71.11%\n",
      "Epoch 2, Batch 4400: Loss = 0.6983, Acc = 71.23%\n",
      "Epoch 2, Batch 4450: Loss = 0.4808, Acc = 71.33%\n",
      "Epoch 2, Batch 4500: Loss = 0.3655, Acc = 71.46%\n",
      "Epoch 2, Batch 4550: Loss = 0.3971, Acc = 71.57%\n",
      "Epoch 2, Batch 4600: Loss = 0.2871, Acc = 71.68%\n",
      "Epoch 2, Batch 4650: Loss = 0.2877, Acc = 71.78%\n",
      "Epoch 2, Batch 4700: Loss = 0.5058, Acc = 71.88%\n",
      "Epoch 2, Batch 4750: Loss = 0.3658, Acc = 71.98%\n",
      "Epoch 2, Batch 4800: Loss = 0.4541, Acc = 72.10%\n",
      "Epoch 2, Batch 4850: Loss = 0.4697, Acc = 72.17%\n",
      "Epoch 2, Batch 4900: Loss = 0.3533, Acc = 72.29%\n",
      "Epoch 2, Batch 4950: Loss = 0.4198, Acc = 72.41%\n",
      "Epoch 2, Batch 5000: Loss = 1.2302, Acc = 72.51%\n",
      "Epoch 2, Batch 5050: Loss = 0.1470, Acc = 72.60%\n",
      "Epoch 2, Batch 5100: Loss = 0.3471, Acc = 72.68%\n",
      "Epoch 2, Batch 5150: Loss = 0.5786, Acc = 72.76%\n",
      "Epoch 2, Batch 5200: Loss = 0.3573, Acc = 72.88%\n",
      "Epoch 2, Batch 5250: Loss = 0.2595, Acc = 72.98%\n",
      "Epoch 2, Batch 5300: Loss = 0.7616, Acc = 73.07%\n",
      "Epoch 2, Batch 5350: Loss = 0.4707, Acc = 73.16%\n",
      "Epoch 2, Batch 5400: Loss = 0.3917, Acc = 73.26%\n",
      "Epoch 2, Batch 5450: Loss = 0.5367, Acc = 73.33%\n",
      "Epoch 2, Batch 5500: Loss = 0.6548, Acc = 73.42%\n",
      "Epoch 2, Batch 5550: Loss = 0.2320, Acc = 73.52%\n",
      "Epoch 2, Batch 5600: Loss = 0.3364, Acc = 73.62%\n",
      "Epoch 2, Batch 5650: Loss = 0.3184, Acc = 73.70%\n",
      "Epoch 2, Batch 5700: Loss = 0.7264, Acc = 73.79%\n",
      "Epoch 2, Batch 5750: Loss = 0.4162, Acc = 73.88%\n",
      "Epoch 2, Batch 5800: Loss = 0.2127, Acc = 73.96%\n",
      "Epoch 2, Batch 5850: Loss = 0.1775, Acc = 74.07%\n",
      "Epoch 2, Batch 5900: Loss = 0.2596, Acc = 74.16%\n",
      "Epoch 2, Batch 5950: Loss = 0.4444, Acc = 74.23%\n",
      "Epoch 2, Batch 6000: Loss = 0.6018, Acc = 74.31%\n",
      "Epoch 2, Batch 6050: Loss = 0.3548, Acc = 74.37%\n",
      "Epoch 2, Batch 6100: Loss = 0.4743, Acc = 74.44%\n",
      "Epoch 2, Batch 6150: Loss = 0.2485, Acc = 74.53%\n",
      "Epoch 2, Batch 6200: Loss = 0.6555, Acc = 74.60%\n",
      "Epoch 2, Batch 6250: Loss = 0.2458, Acc = 74.68%\n",
      "Epoch 2, Batch 6300: Loss = 0.5482, Acc = 74.75%\n",
      "Epoch 2, Batch 6350: Loss = 0.1920, Acc = 74.84%\n",
      "Epoch 2, Batch 6400: Loss = 0.3902, Acc = 74.91%\n",
      "Epoch 2, Batch 6450: Loss = 0.6905, Acc = 74.98%\n",
      "Epoch 2, Batch 6500: Loss = 0.7789, Acc = 75.04%\n",
      "Epoch 2, Batch 6550: Loss = 0.6841, Acc = 75.10%\n",
      "Epoch 2, Batch 6600: Loss = 0.7240, Acc = 75.18%\n",
      "Epoch 2, Batch 6650: Loss = 0.3623, Acc = 75.23%\n",
      "Epoch 2, Batch 6700: Loss = 0.4080, Acc = 75.32%\n",
      "Epoch 2, Batch 6750: Loss = 0.1782, Acc = 75.40%\n",
      "Epoch 2, Batch 6800: Loss = 0.3606, Acc = 75.47%\n",
      "Epoch 2, Batch 6850: Loss = 0.2823, Acc = 75.54%\n",
      "Epoch 2, Batch 6900: Loss = 0.2778, Acc = 75.60%\n",
      "Epoch 2, Batch 6950: Loss = 0.3222, Acc = 75.66%\n",
      "Epoch 2, Batch 7000: Loss = 0.3057, Acc = 75.74%\n",
      "Epoch 2, Batch 7050: Loss = 0.1718, Acc = 75.80%\n",
      "Epoch 2, Batch 7100: Loss = 0.2666, Acc = 75.87%\n",
      "Epoch 2, Batch 7150: Loss = 0.6987, Acc = 75.94%\n",
      "Epoch 2, Batch 7200: Loss = 0.1918, Acc = 76.01%\n",
      "Epoch 2, Batch 7250: Loss = 1.0649, Acc = 76.09%\n",
      "Epoch 2, Batch 7300: Loss = 0.4939, Acc = 76.17%\n",
      "Epoch 2, Batch 7350: Loss = 0.5901, Acc = 76.23%\n",
      "Epoch 2, Batch 7400: Loss = 0.1809, Acc = 76.30%\n",
      "Epoch 2, Batch 7450: Loss = 0.3731, Acc = 76.35%\n",
      "Epoch 2/5:\n",
      "  Train - Loss: 0.5458, Acc: 0.7640\n",
      "  Test  - Loss: 0.4600, Acc: 0.8609\n",
      "  Time: 217.47s, LR: 2.00e-05\n",
      "  Scaler Scale: 16384.0\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 3, Batch 0: Loss = 0.5397, Acc = 81.25%\n",
      "Epoch 3, Batch 50: Loss = 0.3357, Acc = 86.27%\n",
      "Epoch 3, Batch 100: Loss = 0.4529, Acc = 86.70%\n",
      "Epoch 3, Batch 150: Loss = 0.4140, Acc = 85.89%\n",
      "Epoch 3, Batch 200: Loss = 0.3532, Acc = 85.85%\n",
      "Epoch 3, Batch 250: Loss = 0.4889, Acc = 85.83%\n",
      "Epoch 3, Batch 300: Loss = 0.1552, Acc = 85.74%\n",
      "Epoch 3, Batch 350: Loss = 0.4445, Acc = 86.06%\n",
      "Epoch 3, Batch 400: Loss = 0.3256, Acc = 85.91%\n",
      "Epoch 3, Batch 450: Loss = 0.7446, Acc = 86.16%\n",
      "Epoch 3, Batch 500: Loss = 0.0973, Acc = 86.00%\n",
      "Epoch 3, Batch 550: Loss = 0.6331, Acc = 85.99%\n",
      "Epoch 3, Batch 600: Loss = 0.2725, Acc = 86.03%\n",
      "Epoch 3, Batch 650: Loss = 0.1627, Acc = 86.01%\n",
      "Epoch 3, Batch 700: Loss = 0.0991, Acc = 86.02%\n",
      "Epoch 3, Batch 750: Loss = 0.2571, Acc = 85.94%\n",
      "Epoch 3, Batch 800: Loss = 0.6388, Acc = 85.96%\n",
      "Epoch 3, Batch 850: Loss = 0.1972, Acc = 85.97%\n",
      "Epoch 3, Batch 900: Loss = 0.2509, Acc = 85.95%\n",
      "Epoch 3, Batch 950: Loss = 0.1679, Acc = 86.13%\n",
      "Epoch 3, Batch 1000: Loss = 0.4802, Acc = 86.18%\n",
      "Epoch 3, Batch 1050: Loss = 0.1280, Acc = 86.11%\n",
      "Epoch 3, Batch 1100: Loss = 0.5375, Acc = 86.21%\n",
      "Epoch 3, Batch 1150: Loss = 0.1578, Acc = 86.29%\n",
      "Epoch 3, Batch 1200: Loss = 0.6743, Acc = 86.25%\n",
      "Epoch 3, Batch 1250: Loss = 0.5185, Acc = 86.29%\n",
      "Epoch 3, Batch 1300: Loss = 0.4985, Acc = 86.32%\n",
      "Epoch 3, Batch 1350: Loss = 0.2640, Acc = 86.29%\n",
      "Epoch 3, Batch 1400: Loss = 0.5088, Acc = 86.20%\n",
      "Epoch 3, Batch 1450: Loss = 0.7922, Acc = 86.22%\n",
      "Epoch 3, Batch 1500: Loss = 0.1795, Acc = 86.21%\n",
      "Epoch 3, Batch 1550: Loss = 0.5768, Acc = 86.26%\n",
      "Epoch 3, Batch 1600: Loss = 0.3402, Acc = 86.25%\n",
      "Epoch 3, Batch 1650: Loss = 0.5725, Acc = 86.32%\n",
      "Epoch 3, Batch 1700: Loss = 0.3878, Acc = 86.34%\n",
      "Epoch 3, Batch 1750: Loss = 0.2060, Acc = 86.39%\n",
      "Epoch 3, Batch 1800: Loss = 0.0777, Acc = 86.44%\n",
      "Epoch 3, Batch 1850: Loss = 0.2329, Acc = 86.47%\n",
      "Epoch 3, Batch 1900: Loss = 0.2343, Acc = 86.47%\n",
      "Epoch 3, Batch 1950: Loss = 0.0972, Acc = 86.52%\n",
      "Epoch 3, Batch 2000: Loss = 0.1572, Acc = 86.57%\n",
      "Epoch 3, Batch 2050: Loss = 0.0515, Acc = 86.62%\n",
      "Epoch 3, Batch 2100: Loss = 0.4075, Acc = 86.63%\n",
      "Epoch 3, Batch 2150: Loss = 0.2204, Acc = 86.67%\n",
      "Epoch 3, Batch 2200: Loss = 0.8547, Acc = 86.63%\n",
      "Epoch 3, Batch 2250: Loss = 0.2437, Acc = 86.70%\n",
      "Epoch 3, Batch 2300: Loss = 0.3625, Acc = 86.70%\n",
      "Epoch 3, Batch 2350: Loss = 0.2183, Acc = 86.71%\n",
      "Epoch 3, Batch 2400: Loss = 0.2701, Acc = 86.70%\n",
      "Epoch 3, Batch 2450: Loss = 0.3099, Acc = 86.71%\n",
      "Epoch 3, Batch 2500: Loss = 0.1931, Acc = 86.70%\n",
      "Epoch 3, Batch 2550: Loss = 0.2643, Acc = 86.74%\n",
      "Epoch 3, Batch 2600: Loss = 0.4196, Acc = 86.72%\n",
      "Epoch 3, Batch 2650: Loss = 0.2491, Acc = 86.74%\n",
      "Epoch 3, Batch 2700: Loss = 0.6870, Acc = 86.76%\n",
      "Epoch 3, Batch 2750: Loss = 0.6153, Acc = 86.77%\n",
      "Epoch 3, Batch 2800: Loss = 0.3425, Acc = 86.76%\n",
      "Epoch 3, Batch 2850: Loss = 0.2789, Acc = 86.76%\n",
      "Epoch 3, Batch 2900: Loss = 0.2046, Acc = 86.79%\n",
      "Epoch 3, Batch 2950: Loss = 0.8100, Acc = 86.78%\n",
      "Epoch 3, Batch 3000: Loss = 0.2520, Acc = 86.79%\n",
      "Epoch 3, Batch 3050: Loss = 0.4961, Acc = 86.79%\n",
      "Epoch 3, Batch 3100: Loss = 0.1731, Acc = 86.80%\n",
      "Epoch 3, Batch 3150: Loss = 0.0593, Acc = 86.82%\n",
      "Epoch 3, Batch 3200: Loss = 0.0878, Acc = 86.84%\n",
      "Epoch 3, Batch 3250: Loss = 0.6972, Acc = 86.85%\n",
      "Epoch 3, Batch 3300: Loss = 0.5643, Acc = 86.85%\n",
      "Epoch 3, Batch 3350: Loss = 0.0662, Acc = 86.87%\n",
      "Epoch 3, Batch 3400: Loss = 0.2761, Acc = 86.90%\n",
      "Epoch 3, Batch 3450: Loss = 1.0465, Acc = 86.90%\n",
      "Epoch 3, Batch 3500: Loss = 0.2291, Acc = 86.95%\n",
      "Epoch 3, Batch 3550: Loss = 0.7472, Acc = 86.96%\n",
      "Epoch 3, Batch 3600: Loss = 0.4886, Acc = 86.95%\n",
      "Epoch 3, Batch 3650: Loss = 0.0729, Acc = 86.96%\n",
      "Epoch 3, Batch 3700: Loss = 0.1397, Acc = 86.96%\n",
      "Epoch 3, Batch 3750: Loss = 0.3590, Acc = 86.93%\n",
      "Epoch 3, Batch 3800: Loss = 0.1643, Acc = 86.94%\n",
      "Epoch 3, Batch 3850: Loss = 0.3578, Acc = 86.95%\n",
      "Epoch 3, Batch 3900: Loss = 0.4139, Acc = 86.97%\n",
      "Epoch 3, Batch 3950: Loss = 0.1017, Acc = 86.99%\n",
      "Epoch 3, Batch 4000: Loss = 0.4152, Acc = 86.99%\n",
      "Epoch 3, Batch 4050: Loss = 0.1837, Acc = 87.02%\n",
      "Epoch 3, Batch 4100: Loss = 0.2307, Acc = 87.01%\n",
      "Epoch 3, Batch 4150: Loss = 0.3018, Acc = 87.03%\n",
      "Epoch 3, Batch 4200: Loss = 0.1642, Acc = 87.03%\n",
      "Epoch 3, Batch 4250: Loss = 0.2794, Acc = 87.05%\n",
      "Epoch 3, Batch 4300: Loss = 0.4278, Acc = 87.06%\n",
      "Epoch 3, Batch 4350: Loss = 0.0929, Acc = 87.06%\n",
      "Epoch 3, Batch 4400: Loss = 0.5239, Acc = 87.06%\n",
      "Epoch 3, Batch 4450: Loss = 0.1431, Acc = 87.08%\n",
      "Epoch 3, Batch 4500: Loss = 0.4170, Acc = 87.11%\n",
      "Epoch 3, Batch 4550: Loss = 0.3323, Acc = 87.15%\n",
      "Epoch 3, Batch 4600: Loss = 0.8424, Acc = 87.15%\n",
      "Epoch 3, Batch 4650: Loss = 0.2622, Acc = 87.17%\n",
      "Epoch 3, Batch 4700: Loss = 0.5071, Acc = 87.17%\n",
      "Epoch 3, Batch 4750: Loss = 0.3585, Acc = 87.18%\n",
      "Epoch 3, Batch 4800: Loss = 0.2455, Acc = 87.18%\n",
      "Epoch 3, Batch 4850: Loss = 0.3683, Acc = 87.17%\n",
      "Epoch 3, Batch 4900: Loss = 0.1993, Acc = 87.19%\n",
      "Epoch 3, Batch 4950: Loss = 0.1726, Acc = 87.19%\n",
      "Epoch 3, Batch 5000: Loss = 0.1098, Acc = 87.19%\n",
      "Epoch 3, Batch 5050: Loss = 0.3918, Acc = 87.20%\n",
      "Epoch 3, Batch 5100: Loss = 0.2400, Acc = 87.20%\n",
      "Epoch 3, Batch 5150: Loss = 0.4910, Acc = 87.19%\n",
      "Epoch 3, Batch 5200: Loss = 0.0386, Acc = 87.21%\n",
      "Epoch 3, Batch 5250: Loss = 0.3307, Acc = 87.20%\n",
      "Epoch 3, Batch 5300: Loss = 0.5207, Acc = 87.20%\n",
      "Epoch 3, Batch 5350: Loss = 0.2251, Acc = 87.20%\n",
      "Epoch 3, Batch 5400: Loss = 0.2107, Acc = 87.21%\n",
      "Epoch 3, Batch 5450: Loss = 0.1073, Acc = 87.22%\n",
      "Epoch 3, Batch 5500: Loss = 0.5052, Acc = 87.22%\n",
      "Epoch 3, Batch 5550: Loss = 0.8907, Acc = 87.21%\n",
      "Epoch 3, Batch 5600: Loss = 0.0792, Acc = 87.22%\n",
      "Epoch 3, Batch 5650: Loss = 0.2287, Acc = 87.23%\n",
      "Epoch 3, Batch 5700: Loss = 0.3230, Acc = 87.24%\n",
      "Epoch 3, Batch 5750: Loss = 0.4370, Acc = 87.24%\n",
      "Epoch 3, Batch 5800: Loss = 0.3802, Acc = 87.25%\n",
      "Epoch 3, Batch 5850: Loss = 0.2192, Acc = 87.26%\n",
      "Epoch 3, Batch 5900: Loss = 0.0812, Acc = 87.27%\n",
      "Epoch 3, Batch 5950: Loss = 0.3585, Acc = 87.28%\n",
      "Epoch 3, Batch 6000: Loss = 0.0891, Acc = 87.30%\n",
      "Epoch 3, Batch 6050: Loss = 0.1123, Acc = 87.30%\n",
      "Epoch 3, Batch 6100: Loss = 0.2913, Acc = 87.30%\n",
      "Epoch 3, Batch 6150: Loss = 0.3710, Acc = 87.31%\n",
      "Epoch 3, Batch 6200: Loss = 1.2991, Acc = 87.32%\n",
      "Epoch 3, Batch 6250: Loss = 0.2085, Acc = 87.32%\n",
      "Epoch 3, Batch 6300: Loss = 0.4731, Acc = 87.33%\n",
      "Epoch 3, Batch 6350: Loss = 0.6220, Acc = 87.33%\n",
      "Epoch 3, Batch 6400: Loss = 0.1309, Acc = 87.35%\n",
      "Epoch 3, Batch 6450: Loss = 0.2916, Acc = 87.34%\n",
      "Epoch 3, Batch 6500: Loss = 0.4929, Acc = 87.35%\n",
      "Epoch 3, Batch 6550: Loss = 0.5392, Acc = 87.37%\n",
      "Epoch 3, Batch 6600: Loss = 0.0845, Acc = 87.38%\n",
      "Epoch 3, Batch 6650: Loss = 0.1781, Acc = 87.39%\n",
      "Epoch 3, Batch 6700: Loss = 0.1479, Acc = 87.41%\n",
      "Epoch 3, Batch 6750: Loss = 0.4140, Acc = 87.42%\n",
      "Epoch 3, Batch 6800: Loss = 0.1319, Acc = 87.42%\n",
      "Epoch 3, Batch 6850: Loss = 0.2082, Acc = 87.42%\n",
      "Epoch 3, Batch 6900: Loss = 0.4686, Acc = 87.42%\n",
      "Epoch 3, Batch 6950: Loss = 0.4544, Acc = 87.42%\n",
      "Epoch 3, Batch 7000: Loss = 0.2911, Acc = 87.42%\n",
      "Epoch 3, Batch 7050: Loss = 0.3939, Acc = 87.42%\n",
      "Epoch 3, Batch 7100: Loss = 0.2114, Acc = 87.43%\n",
      "Epoch 3, Batch 7150: Loss = 0.5132, Acc = 87.44%\n",
      "Epoch 3, Batch 7200: Loss = 0.0885, Acc = 87.46%\n",
      "Epoch 3, Batch 7250: Loss = 0.4592, Acc = 87.45%\n",
      "Epoch 3, Batch 7300: Loss = 0.4296, Acc = 87.47%\n",
      "Epoch 3, Batch 7350: Loss = 0.3696, Acc = 87.48%\n",
      "Epoch 3, Batch 7400: Loss = 0.1680, Acc = 87.49%\n",
      "Epoch 3, Batch 7450: Loss = 0.2976, Acc = 87.49%\n",
      "Epoch 3/5:\n",
      "  Train - Loss: 0.3652, Acc: 0.8748\n",
      "  Test  - Loss: 0.4214, Acc: 0.8821\n",
      "  Time: 217.76s, LR: 2.00e-05\n",
      "  Scaler Scale: 32768.0\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 4, Batch 0: Loss = 0.8736, Acc = 87.50%\n",
      "Epoch 4, Batch 50: Loss = 0.2076, Acc = 88.97%\n",
      "Epoch 4, Batch 100: Loss = 0.0851, Acc = 88.74%\n",
      "Epoch 4, Batch 150: Loss = 0.2864, Acc = 88.58%\n",
      "Epoch 4, Batch 200: Loss = 0.4325, Acc = 88.56%\n",
      "Epoch 4, Batch 250: Loss = 0.1964, Acc = 88.62%\n",
      "Epoch 4, Batch 300: Loss = 0.5562, Acc = 88.48%\n",
      "Epoch 4, Batch 350: Loss = 0.4832, Acc = 88.57%\n",
      "Epoch 4, Batch 400: Loss = 0.3106, Acc = 88.82%\n",
      "Epoch 4, Batch 450: Loss = 0.4524, Acc = 88.82%\n",
      "Epoch 4, Batch 500: Loss = 0.3011, Acc = 88.83%\n",
      "Epoch 4, Batch 550: Loss = 0.9758, Acc = 88.79%\n",
      "Epoch 4, Batch 600: Loss = 0.1583, Acc = 88.72%\n",
      "Epoch 4, Batch 650: Loss = 0.3175, Acc = 88.60%\n",
      "Epoch 4, Batch 700: Loss = 0.3392, Acc = 88.58%\n",
      "Epoch 4, Batch 750: Loss = 0.3366, Acc = 88.62%\n",
      "Epoch 4, Batch 800: Loss = 0.6471, Acc = 88.72%\n",
      "Epoch 4, Batch 850: Loss = 0.2475, Acc = 88.79%\n",
      "Epoch 4, Batch 900: Loss = 0.2664, Acc = 88.88%\n",
      "Epoch 4, Batch 950: Loss = 0.4051, Acc = 88.76%\n",
      "Epoch 4, Batch 1000: Loss = 0.8341, Acc = 88.72%\n",
      "Epoch 4, Batch 1050: Loss = 0.3354, Acc = 88.67%\n",
      "Epoch 4, Batch 1100: Loss = 0.1090, Acc = 88.69%\n",
      "Epoch 4, Batch 1150: Loss = 0.0704, Acc = 88.72%\n",
      "Epoch 4, Batch 1200: Loss = 0.1027, Acc = 88.69%\n",
      "Epoch 4, Batch 1250: Loss = 0.2968, Acc = 88.71%\n",
      "Epoch 4, Batch 1300: Loss = 0.0461, Acc = 88.80%\n",
      "Epoch 4, Batch 1350: Loss = 0.4481, Acc = 88.85%\n",
      "Epoch 4, Batch 1400: Loss = 0.3268, Acc = 88.87%\n",
      "Epoch 4, Batch 1450: Loss = 0.2138, Acc = 88.81%\n",
      "Epoch 4, Batch 1500: Loss = 0.3735, Acc = 88.86%\n",
      "Epoch 4, Batch 1550: Loss = 0.1965, Acc = 88.91%\n",
      "Epoch 4, Batch 1600: Loss = 0.3496, Acc = 88.94%\n",
      "Epoch 4, Batch 1650: Loss = 0.0413, Acc = 89.00%\n",
      "Epoch 4, Batch 1700: Loss = 0.6340, Acc = 89.02%\n",
      "Epoch 4, Batch 1750: Loss = 0.5025, Acc = 88.98%\n",
      "Epoch 4, Batch 1800: Loss = 0.6776, Acc = 88.98%\n",
      "Epoch 4, Batch 1850: Loss = 0.1562, Acc = 88.99%\n",
      "Epoch 4, Batch 1900: Loss = 0.1836, Acc = 89.00%\n",
      "Epoch 4, Batch 1950: Loss = 0.0640, Acc = 89.03%\n",
      "Epoch 4, Batch 2000: Loss = 0.6741, Acc = 89.01%\n",
      "Epoch 4, Batch 2050: Loss = 0.1839, Acc = 89.05%\n",
      "Epoch 4, Batch 2100: Loss = 0.1577, Acc = 89.06%\n",
      "Epoch 4, Batch 2150: Loss = 0.0945, Acc = 89.03%\n",
      "Epoch 4, Batch 2200: Loss = 0.3005, Acc = 89.09%\n",
      "Epoch 4, Batch 2250: Loss = 0.4639, Acc = 89.09%\n",
      "Epoch 4, Batch 2300: Loss = 0.4190, Acc = 89.05%\n",
      "Epoch 4, Batch 2350: Loss = 0.1272, Acc = 89.06%\n",
      "Epoch 4, Batch 2400: Loss = 0.0857, Acc = 89.09%\n",
      "Epoch 4, Batch 2450: Loss = 0.5160, Acc = 89.08%\n",
      "Epoch 4, Batch 2500: Loss = 0.1067, Acc = 89.11%\n",
      "Epoch 4, Batch 2550: Loss = 0.4194, Acc = 89.09%\n",
      "Epoch 4, Batch 2600: Loss = 0.4577, Acc = 89.10%\n",
      "Epoch 4, Batch 2650: Loss = 0.0907, Acc = 89.12%\n",
      "Epoch 4, Batch 2700: Loss = 0.3778, Acc = 89.13%\n",
      "Epoch 4, Batch 2750: Loss = 0.7365, Acc = 89.14%\n",
      "Epoch 4, Batch 2800: Loss = 0.0663, Acc = 89.17%\n",
      "Epoch 4, Batch 2850: Loss = 0.2092, Acc = 89.16%\n",
      "Epoch 4, Batch 2900: Loss = 0.0560, Acc = 89.15%\n",
      "Epoch 4, Batch 2950: Loss = 0.4005, Acc = 89.18%\n",
      "Epoch 4, Batch 3000: Loss = 0.0301, Acc = 89.20%\n",
      "Epoch 4, Batch 3050: Loss = 0.3761, Acc = 89.21%\n",
      "Epoch 4, Batch 3100: Loss = 0.3480, Acc = 89.23%\n",
      "Epoch 4, Batch 3150: Loss = 0.5083, Acc = 89.22%\n",
      "Epoch 4, Batch 3200: Loss = 0.3059, Acc = 89.23%\n",
      "Epoch 4, Batch 3250: Loss = 0.1123, Acc = 89.26%\n",
      "Epoch 4, Batch 3300: Loss = 0.1699, Acc = 89.23%\n",
      "Epoch 4, Batch 3350: Loss = 0.3986, Acc = 89.25%\n",
      "Epoch 4, Batch 3400: Loss = 0.1892, Acc = 89.25%\n",
      "Epoch 4, Batch 3450: Loss = 0.6830, Acc = 89.24%\n",
      "Epoch 4, Batch 3500: Loss = 0.3389, Acc = 89.24%\n",
      "Epoch 4, Batch 3550: Loss = 0.3629, Acc = 89.23%\n",
      "Epoch 4, Batch 3600: Loss = 0.5370, Acc = 89.21%\n",
      "Epoch 4, Batch 3650: Loss = 0.8892, Acc = 89.22%\n",
      "Epoch 4, Batch 3700: Loss = 0.1589, Acc = 89.23%\n",
      "Epoch 4, Batch 3750: Loss = 0.2725, Acc = 89.24%\n",
      "Epoch 4, Batch 3800: Loss = 1.1270, Acc = 89.21%\n",
      "Epoch 4, Batch 3850: Loss = 0.2824, Acc = 89.22%\n",
      "Epoch 4, Batch 3900: Loss = 0.6981, Acc = 89.19%\n",
      "Epoch 4, Batch 3950: Loss = 0.3024, Acc = 89.19%\n",
      "Epoch 4, Batch 4000: Loss = 0.7072, Acc = 89.19%\n",
      "Epoch 4, Batch 4050: Loss = 0.1960, Acc = 89.20%\n",
      "Epoch 4, Batch 4100: Loss = 0.1620, Acc = 89.19%\n",
      "Epoch 4, Batch 4150: Loss = 0.1664, Acc = 89.17%\n",
      "Epoch 4, Batch 4200: Loss = 0.2786, Acc = 89.19%\n",
      "Epoch 4, Batch 4250: Loss = 0.7902, Acc = 89.22%\n",
      "Epoch 4, Batch 4300: Loss = 0.0373, Acc = 89.21%\n",
      "Epoch 4, Batch 4350: Loss = 0.0902, Acc = 89.22%\n",
      "Epoch 4, Batch 4400: Loss = 0.0576, Acc = 89.23%\n",
      "Epoch 4, Batch 4450: Loss = 0.3664, Acc = 89.25%\n",
      "Epoch 4, Batch 4500: Loss = 0.2586, Acc = 89.26%\n",
      "Epoch 4, Batch 4550: Loss = 0.1606, Acc = 89.25%\n",
      "Epoch 4, Batch 4600: Loss = 0.1180, Acc = 89.26%\n",
      "Epoch 4, Batch 4650: Loss = 0.3902, Acc = 89.25%\n",
      "Epoch 4, Batch 4700: Loss = 0.4786, Acc = 89.25%\n",
      "Epoch 4, Batch 4750: Loss = 0.1637, Acc = 89.26%\n",
      "Epoch 4, Batch 4800: Loss = 0.4598, Acc = 89.27%\n",
      "Epoch 4, Batch 4850: Loss = 0.3268, Acc = 89.26%\n",
      "Epoch 4, Batch 4900: Loss = 0.0742, Acc = 89.26%\n",
      "Epoch 4, Batch 4950: Loss = 0.1028, Acc = 89.27%\n",
      "Epoch 4, Batch 5000: Loss = 0.0940, Acc = 89.26%\n",
      "Epoch 4, Batch 5050: Loss = 0.3294, Acc = 89.26%\n",
      "Epoch 4, Batch 5100: Loss = 0.3174, Acc = 89.23%\n",
      "Epoch 4, Batch 5150: Loss = 0.2855, Acc = 89.24%\n",
      "Epoch 4, Batch 5200: Loss = 0.3758, Acc = 89.23%\n",
      "Epoch 4, Batch 5250: Loss = 0.1413, Acc = 89.22%\n",
      "Epoch 4, Batch 5300: Loss = 0.0243, Acc = 89.24%\n",
      "Epoch 4, Batch 5350: Loss = 0.1043, Acc = 89.23%\n",
      "Epoch 4, Batch 5400: Loss = 0.1390, Acc = 89.24%\n",
      "Epoch 4, Batch 5450: Loss = 0.5735, Acc = 89.24%\n",
      "Epoch 4, Batch 5500: Loss = 0.1417, Acc = 89.21%\n",
      "Epoch 4, Batch 5550: Loss = 0.1036, Acc = 89.22%\n",
      "Epoch 4, Batch 5600: Loss = 0.1069, Acc = 89.24%\n",
      "Epoch 4, Batch 5650: Loss = 0.2582, Acc = 89.24%\n",
      "Epoch 4, Batch 5700: Loss = 0.5471, Acc = 89.25%\n",
      "Epoch 4, Batch 5750: Loss = 0.2048, Acc = 89.24%\n",
      "Epoch 4, Batch 5800: Loss = 0.0970, Acc = 89.24%\n",
      "Epoch 4, Batch 5850: Loss = 0.0311, Acc = 89.25%\n",
      "Epoch 4, Batch 5900: Loss = 0.5877, Acc = 89.25%\n",
      "Epoch 4, Batch 5950: Loss = 0.2450, Acc = 89.25%\n",
      "Epoch 4, Batch 6000: Loss = 0.4584, Acc = 89.24%\n",
      "Epoch 4, Batch 6050: Loss = 0.7746, Acc = 89.24%\n",
      "Epoch 4, Batch 6100: Loss = 0.2210, Acc = 89.25%\n",
      "Epoch 4, Batch 6150: Loss = 0.2244, Acc = 89.26%\n",
      "Epoch 4, Batch 6200: Loss = 0.7717, Acc = 89.27%\n",
      "Epoch 4, Batch 6250: Loss = 0.5359, Acc = 89.26%\n",
      "Epoch 4, Batch 6300: Loss = 0.0904, Acc = 89.26%\n",
      "Epoch 4, Batch 6350: Loss = 0.1495, Acc = 89.27%\n",
      "Epoch 4, Batch 6400: Loss = 0.4982, Acc = 89.28%\n",
      "Epoch 4, Batch 6450: Loss = 0.5280, Acc = 89.28%\n",
      "Epoch 4, Batch 6500: Loss = 0.1241, Acc = 89.29%\n",
      "Epoch 4, Batch 6550: Loss = 0.5217, Acc = 89.29%\n",
      "Epoch 4, Batch 6600: Loss = 0.3328, Acc = 89.29%\n",
      "Epoch 4, Batch 6650: Loss = 0.3908, Acc = 89.29%\n",
      "Epoch 4, Batch 6700: Loss = 0.1838, Acc = 89.31%\n",
      "Epoch 4, Batch 6750: Loss = 0.5712, Acc = 89.32%\n",
      "Epoch 4, Batch 6800: Loss = 0.1975, Acc = 89.32%\n",
      "Epoch 4, Batch 6850: Loss = 0.3951, Acc = 89.33%\n",
      "Epoch 4, Batch 6900: Loss = 0.2186, Acc = 89.33%\n",
      "Epoch 4, Batch 6950: Loss = 0.4672, Acc = 89.34%\n",
      "Epoch 4, Batch 7000: Loss = 0.5658, Acc = 89.35%\n",
      "Epoch 4, Batch 7050: Loss = 0.5771, Acc = 89.36%\n",
      "Epoch 4, Batch 7100: Loss = 0.5025, Acc = 89.35%\n",
      "Epoch 4, Batch 7150: Loss = 0.0972, Acc = 89.36%\n",
      "Epoch 4, Batch 7200: Loss = 0.2936, Acc = 89.36%\n",
      "Epoch 4, Batch 7250: Loss = 0.5624, Acc = 89.35%\n",
      "Epoch 4, Batch 7300: Loss = 0.6576, Acc = 89.35%\n",
      "Epoch 4, Batch 7350: Loss = 0.3774, Acc = 89.35%\n",
      "Epoch 4, Batch 7400: Loss = 0.7711, Acc = 89.35%\n",
      "Epoch 4, Batch 7450: Loss = 0.5139, Acc = 89.35%\n",
      "Epoch 4/5:\n",
      "  Train - Loss: 0.3231, Acc: 0.8935\n",
      "  Test  - Loss: 0.4233, Acc: 0.8924\n",
      "  Time: 217.44s, LR: 2.00e-05\n",
      "  Scaler Scale: 32768.0\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 5, Batch 0: Loss = 0.2843, Acc = 81.25%\n",
      "Epoch 5, Batch 50: Loss = 0.4335, Acc = 89.58%\n",
      "Epoch 5, Batch 100: Loss = 0.1279, Acc = 90.28%\n",
      "Epoch 5, Batch 150: Loss = 0.0685, Acc = 90.52%\n",
      "Epoch 5, Batch 200: Loss = 0.0787, Acc = 90.73%\n",
      "Epoch 5, Batch 250: Loss = 0.5509, Acc = 90.36%\n",
      "Epoch 5, Batch 300: Loss = 0.0902, Acc = 90.64%\n",
      "Epoch 5, Batch 350: Loss = 0.0893, Acc = 90.38%\n",
      "Epoch 5, Batch 400: Loss = 0.1053, Acc = 90.31%\n",
      "Epoch 5, Batch 450: Loss = 0.3811, Acc = 90.05%\n",
      "Epoch 5, Batch 500: Loss = 0.3475, Acc = 90.19%\n",
      "Epoch 5, Batch 550: Loss = 0.4193, Acc = 90.29%\n",
      "Epoch 5, Batch 600: Loss = 0.1622, Acc = 90.28%\n",
      "Epoch 5, Batch 650: Loss = 0.5732, Acc = 90.27%\n",
      "Epoch 5, Batch 700: Loss = 0.7065, Acc = 90.42%\n",
      "Epoch 5, Batch 750: Loss = 0.2527, Acc = 90.41%\n",
      "Epoch 5, Batch 800: Loss = 0.4614, Acc = 90.40%\n",
      "Epoch 5, Batch 850: Loss = 0.1050, Acc = 90.36%\n",
      "Epoch 5, Batch 900: Loss = 0.1834, Acc = 90.37%\n",
      "Epoch 5, Batch 950: Loss = 0.3727, Acc = 90.25%\n",
      "Epoch 5, Batch 1000: Loss = 0.4052, Acc = 90.33%\n",
      "Epoch 5, Batch 1050: Loss = 0.3465, Acc = 90.34%\n",
      "Epoch 5, Batch 1100: Loss = 0.0827, Acc = 90.32%\n",
      "Epoch 5, Batch 1150: Loss = 0.2648, Acc = 90.35%\n",
      "Epoch 5, Batch 1200: Loss = 0.2969, Acc = 90.35%\n",
      "Epoch 5, Batch 1250: Loss = 0.5244, Acc = 90.36%\n",
      "Epoch 5, Batch 1300: Loss = 0.4588, Acc = 90.34%\n",
      "Epoch 5, Batch 1350: Loss = 0.7010, Acc = 90.35%\n",
      "Epoch 5, Batch 1400: Loss = 0.1423, Acc = 90.32%\n",
      "Epoch 5, Batch 1450: Loss = 0.3426, Acc = 90.39%\n",
      "Epoch 5, Batch 1500: Loss = 0.1502, Acc = 90.38%\n",
      "Epoch 5, Batch 1550: Loss = 0.0541, Acc = 90.38%\n",
      "Epoch 5, Batch 1600: Loss = 0.2093, Acc = 90.36%\n",
      "Epoch 5, Batch 1650: Loss = 0.0205, Acc = 90.37%\n",
      "Epoch 5, Batch 1700: Loss = 0.1054, Acc = 90.34%\n",
      "Epoch 5, Batch 1750: Loss = 0.4138, Acc = 90.38%\n",
      "Epoch 5, Batch 1800: Loss = 0.1087, Acc = 90.39%\n",
      "Epoch 5, Batch 1850: Loss = 0.0577, Acc = 90.40%\n",
      "Epoch 5, Batch 1900: Loss = 0.4110, Acc = 90.37%\n",
      "Epoch 5, Batch 1950: Loss = 0.1253, Acc = 90.34%\n",
      "Epoch 5, Batch 2000: Loss = 0.2965, Acc = 90.35%\n",
      "Epoch 5, Batch 2050: Loss = 0.5307, Acc = 90.33%\n",
      "Epoch 5, Batch 2100: Loss = 0.2663, Acc = 90.33%\n",
      "Epoch 5, Batch 2150: Loss = 0.0824, Acc = 90.35%\n",
      "Epoch 5, Batch 2200: Loss = 0.2599, Acc = 90.35%\n",
      "Epoch 5, Batch 2250: Loss = 0.1279, Acc = 90.34%\n",
      "Epoch 5, Batch 2300: Loss = 0.3649, Acc = 90.37%\n",
      "Epoch 5, Batch 2350: Loss = 0.3901, Acc = 90.39%\n",
      "Epoch 5, Batch 2400: Loss = 0.3167, Acc = 90.41%\n",
      "Epoch 5, Batch 2450: Loss = 0.1817, Acc = 90.42%\n",
      "Epoch 5, Batch 2500: Loss = 0.0776, Acc = 90.40%\n",
      "Epoch 5, Batch 2550: Loss = 0.4698, Acc = 90.42%\n",
      "Epoch 5, Batch 2600: Loss = 0.0419, Acc = 90.42%\n",
      "Epoch 5, Batch 2650: Loss = 0.4516, Acc = 90.40%\n",
      "Epoch 5, Batch 2700: Loss = 0.3435, Acc = 90.41%\n",
      "Epoch 5, Batch 2750: Loss = 0.1301, Acc = 90.45%\n",
      "Epoch 5, Batch 2800: Loss = 0.0947, Acc = 90.44%\n",
      "Epoch 5, Batch 2850: Loss = 0.4723, Acc = 90.43%\n",
      "Epoch 5, Batch 2900: Loss = 0.1751, Acc = 90.43%\n",
      "Epoch 5, Batch 2950: Loss = 0.0880, Acc = 90.44%\n",
      "Epoch 5, Batch 3000: Loss = 0.4812, Acc = 90.43%\n",
      "Epoch 5, Batch 3050: Loss = 0.0606, Acc = 90.41%\n",
      "Epoch 5, Batch 3100: Loss = 0.3447, Acc = 90.42%\n",
      "Epoch 5, Batch 3150: Loss = 0.3248, Acc = 90.45%\n",
      "Epoch 5, Batch 3200: Loss = 0.5038, Acc = 90.44%\n",
      "Epoch 5, Batch 3250: Loss = 0.0593, Acc = 90.44%\n",
      "Epoch 5, Batch 3300: Loss = 0.0413, Acc = 90.45%\n",
      "Epoch 5, Batch 3350: Loss = 0.0707, Acc = 90.43%\n",
      "Epoch 5, Batch 3400: Loss = 0.3107, Acc = 90.45%\n",
      "Epoch 5, Batch 3450: Loss = 0.3229, Acc = 90.45%\n",
      "Epoch 5, Batch 3500: Loss = 0.4055, Acc = 90.44%\n",
      "Epoch 5, Batch 3550: Loss = 0.2150, Acc = 90.47%\n",
      "Epoch 5, Batch 3600: Loss = 0.1812, Acc = 90.48%\n",
      "Epoch 5, Batch 3650: Loss = 0.5031, Acc = 90.47%\n",
      "Epoch 5, Batch 3700: Loss = 0.3929, Acc = 90.48%\n",
      "Epoch 5, Batch 3750: Loss = 0.2500, Acc = 90.48%\n",
      "Epoch 5, Batch 3800: Loss = 0.4018, Acc = 90.49%\n",
      "Epoch 5, Batch 3850: Loss = 0.4599, Acc = 90.48%\n",
      "Epoch 5, Batch 3900: Loss = 0.1553, Acc = 90.50%\n",
      "Epoch 5, Batch 3950: Loss = 0.1915, Acc = 90.51%\n",
      "Epoch 5, Batch 4000: Loss = 0.5426, Acc = 90.49%\n",
      "Epoch 5, Batch 4050: Loss = 0.2657, Acc = 90.50%\n",
      "Epoch 5, Batch 4100: Loss = 0.2416, Acc = 90.51%\n",
      "Epoch 5, Batch 4150: Loss = 0.0820, Acc = 90.53%\n",
      "Epoch 5, Batch 4200: Loss = 0.2166, Acc = 90.52%\n",
      "Epoch 5, Batch 4250: Loss = 0.5463, Acc = 90.51%\n",
      "Epoch 5, Batch 4300: Loss = 0.2009, Acc = 90.53%\n",
      "Epoch 5, Batch 4350: Loss = 0.0716, Acc = 90.52%\n",
      "Epoch 5, Batch 4400: Loss = 0.3528, Acc = 90.51%\n",
      "Epoch 5, Batch 4450: Loss = 0.1065, Acc = 90.50%\n",
      "Epoch 5, Batch 4500: Loss = 0.3088, Acc = 90.50%\n",
      "Epoch 5, Batch 4550: Loss = 0.5399, Acc = 90.50%\n",
      "Epoch 5, Batch 4600: Loss = 0.5940, Acc = 90.51%\n",
      "Epoch 5, Batch 4650: Loss = 0.5957, Acc = 90.52%\n",
      "Epoch 5, Batch 4700: Loss = 0.6903, Acc = 90.52%\n",
      "Epoch 5, Batch 4750: Loss = 0.0853, Acc = 90.53%\n",
      "Epoch 5, Batch 4800: Loss = 0.5242, Acc = 90.54%\n",
      "Epoch 5, Batch 4850: Loss = 0.4402, Acc = 90.54%\n",
      "Epoch 5, Batch 4900: Loss = 0.6331, Acc = 90.52%\n",
      "Epoch 5, Batch 4950: Loss = 0.8741, Acc = 90.52%\n",
      "Epoch 5, Batch 5000: Loss = 0.5821, Acc = 90.51%\n",
      "Epoch 5, Batch 5050: Loss = 0.1229, Acc = 90.52%\n",
      "Epoch 5, Batch 5100: Loss = 0.3108, Acc = 90.52%\n",
      "Epoch 5, Batch 5150: Loss = 0.0802, Acc = 90.53%\n",
      "Epoch 5, Batch 5200: Loss = 0.1924, Acc = 90.53%\n",
      "Epoch 5, Batch 5250: Loss = 0.1654, Acc = 90.53%\n",
      "Epoch 5, Batch 5300: Loss = 0.7052, Acc = 90.55%\n",
      "Epoch 5, Batch 5350: Loss = 0.2845, Acc = 90.57%\n",
      "Epoch 5, Batch 5400: Loss = 0.1613, Acc = 90.58%\n",
      "Epoch 5, Batch 5450: Loss = 0.0510, Acc = 90.57%\n",
      "Epoch 5, Batch 5500: Loss = 0.1956, Acc = 90.59%\n",
      "Epoch 5, Batch 5550: Loss = 0.2922, Acc = 90.58%\n",
      "Epoch 5, Batch 5600: Loss = 0.2541, Acc = 90.59%\n",
      "Epoch 5, Batch 5650: Loss = 0.0956, Acc = 90.60%\n",
      "Epoch 5, Batch 5700: Loss = 0.0536, Acc = 90.60%\n",
      "Epoch 5, Batch 5750: Loss = 0.6597, Acc = 90.61%\n",
      "Epoch 5, Batch 5800: Loss = 0.1844, Acc = 90.61%\n",
      "Epoch 5, Batch 5850: Loss = 0.1002, Acc = 90.61%\n",
      "Epoch 5, Batch 5900: Loss = 0.5388, Acc = 90.61%\n",
      "Epoch 5, Batch 5950: Loss = 0.0652, Acc = 90.60%\n",
      "Epoch 5, Batch 6000: Loss = 0.4158, Acc = 90.60%\n",
      "Epoch 5, Batch 6050: Loss = 0.1691, Acc = 90.60%\n",
      "Epoch 5, Batch 6100: Loss = 0.6506, Acc = 90.59%\n",
      "Epoch 5, Batch 6150: Loss = 0.2684, Acc = 90.58%\n",
      "Epoch 5, Batch 6200: Loss = 0.1929, Acc = 90.59%\n",
      "Epoch 5, Batch 6250: Loss = 0.1009, Acc = 90.59%\n",
      "Epoch 5, Batch 6300: Loss = 0.1324, Acc = 90.60%\n",
      "Epoch 5, Batch 6350: Loss = 0.6905, Acc = 90.60%\n",
      "Epoch 5, Batch 6400: Loss = 0.2389, Acc = 90.60%\n",
      "Epoch 5, Batch 6450: Loss = 0.2705, Acc = 90.60%\n",
      "Epoch 5, Batch 6500: Loss = 0.2297, Acc = 90.61%\n",
      "Epoch 5, Batch 6550: Loss = 0.2297, Acc = 90.62%\n",
      "Epoch 5, Batch 6600: Loss = 0.3492, Acc = 90.62%\n",
      "Epoch 5, Batch 6650: Loss = 0.1233, Acc = 90.63%\n",
      "Epoch 5, Batch 6700: Loss = 0.1742, Acc = 90.63%\n",
      "Epoch 5, Batch 6750: Loss = 0.4180, Acc = 90.63%\n",
      "Epoch 5, Batch 6800: Loss = 0.1163, Acc = 90.65%\n",
      "Epoch 5, Batch 6850: Loss = 0.3937, Acc = 90.63%\n",
      "Epoch 5, Batch 6900: Loss = 0.2339, Acc = 90.64%\n",
      "Epoch 5, Batch 6950: Loss = 0.1217, Acc = 90.63%\n",
      "Epoch 5, Batch 7000: Loss = 0.3321, Acc = 90.62%\n",
      "Epoch 5, Batch 7050: Loss = 0.4421, Acc = 90.63%\n",
      "Epoch 5, Batch 7100: Loss = 0.1388, Acc = 90.63%\n",
      "Epoch 5, Batch 7150: Loss = 0.1439, Acc = 90.63%\n",
      "Epoch 5, Batch 7200: Loss = 0.0834, Acc = 90.63%\n",
      "Epoch 5, Batch 7250: Loss = 0.1754, Acc = 90.63%\n",
      "Epoch 5, Batch 7300: Loss = 0.3218, Acc = 90.63%\n",
      "Epoch 5, Batch 7350: Loss = 0.5546, Acc = 90.63%\n",
      "Epoch 5, Batch 7400: Loss = 0.1835, Acc = 90.63%\n",
      "Epoch 5, Batch 7450: Loss = 0.3232, Acc = 90.63%\n",
      "Epoch 5/5:\n",
      "  Train - Loss: 0.2915, Acc: 0.9063\n",
      "  Test  - Loss: 0.4182, Acc: 0.9007\n",
      "  Time: 217.82s, LR: 2.00e-05\n",
      "  Scaler Scale: 32768.0\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImprovedDiffusionAttentionFreeModel(\n",
       "  (embedding): Embedding(30522, 256)\n",
       "  (neighbor_proj): ModuleList(\n",
       "    (0-2): 3 x Linear(in_features=256, out_features=256, bias=False)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (update_mlp): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler  # Updated API\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "train(train_loader, test_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162bad96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
