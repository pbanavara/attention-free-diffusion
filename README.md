This is an experiment. Trying out alternatives to
self-attention, for either linear or sub linear memory scaling. Early experimental
results and a rough draft of the paper are in the iPython notebook.
